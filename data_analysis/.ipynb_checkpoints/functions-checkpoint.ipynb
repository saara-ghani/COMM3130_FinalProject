{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1639f96-38ce-4ff2-953b-0f062750f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97f7def-cd1b-44b7-b003-fefe86356a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=False, stripchars=''):\n",
    "    \n",
    "    rdict = str.maketrans('','',stripchars)\n",
    "    \n",
    "    text_norm = text.translate(rdict)\n",
    "    if lowercase:\n",
    "        text_norm = text_norm.lower()\n",
    "        \n",
    "    tokens = text_norm.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b517b4-1c34-479a-b999-a22993003d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freq(freq, size, base=10000):\n",
    "    '''normalize the frequency of an item based on the size of the text/corpus using a base, e.g. per 10,000 words\n",
    "    \n",
    "    Args:\n",
    "        freq   --  the frequency of the item\n",
    "        size   --  the size (number of tokens) in the text/corpus\n",
    "        base   --  normalization unit (DEFAULT: 10,000 tokens)\n",
    "    \n",
    "    Returns:\n",
    "        normalized frequency\n",
    "    \n",
    "    '''\n",
    "    norm_freq = freq/size * base\n",
    "    return norm_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d95a6f-b301-41e3-9ea4-8a0c619b782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kwic(kw, texts, win=4):\n",
    "    '''A basic KWIC function for a text\n",
    "    \n",
    "    Args:\n",
    "        kw   -- string match for keyword to match for each line\n",
    "        texts -- a list of lists of texts in the corpus\n",
    "        \n",
    "    Return:\n",
    "        list of lines of form [ [left context words], kw, [right context words]]\n",
    "    '''\n",
    "    \n",
    "    tokens = [tokenize(text, lowercase = True, stripchars = chars_to_strip)\n",
    "                 for text in texts\n",
    "             ]\n",
    "    \n",
    "    hits = [(word,idx, tidx) for tidx, text in enumerate(tokens) \n",
    "                                for idx,word in enumerate(text) if word==kw]\n",
    "    lines = []\n",
    "    for hit in hits:\n",
    "        text = tokens[hit[2]]\n",
    "        left = text[max(0,hit[1]-win):hit[1]]\n",
    "        kw = text[hit[1]]\n",
    "        right = text[hit[1]+1 : min(len(text),hit[1]+win)+1]\n",
    "        \n",
    "        if len(left)<win:\n",
    "            left = ['']*(win-len(left)) + left\n",
    "            \n",
    "        if len(right)<win:\n",
    "            right = right + ['']*(win-len(right))\n",
    "        \n",
    "        lines.append([left, kw, right])\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceeb3bf-e0a6-42bd-8529-e13f0d9dcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_kwic(kwic, order=None):\n",
    "    ''' sort a kwic list using the passed positional arguments \n",
    "    \n",
    "    Args:\n",
    "        kwic   -- a list of lists [ [left tokens], kw, [right tokens]]\n",
    "        order  -- a list of one or more positional arguments of form side-pos, e.g. L1, R3, L4 (default: None)\n",
    "    \n",
    "    Returns:\n",
    "        kwic sorted for each positional argument in reverse, i.e. ['R1','L1'] sorts first by L1 and then R1\n",
    "    '''\n",
    "    if order is None:\n",
    "        return kwic\n",
    "   \n",
    "    order = [order] if not type(order) is list else order\n",
    "    order.reverse()\n",
    "    \n",
    "    win = len(kwic[0][0])-1\n",
    "    \n",
    "    \n",
    "    for sort_term in order:\n",
    "        if not re.match('[LR][1-4]', sort_term):\n",
    "            pass\n",
    "        \n",
    "        pos1 = 0 if sort_term[0]=='L' else 2\n",
    "        pos2 = int(sort_term[1])-1\n",
    "        pos2 = win-pos2 if sort_term[0]=='L' else pos2\n",
    "        \n",
    "        kwic.sort(key=lambda l : l[pos1][pos2])\n",
    "    \n",
    "    return kwic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e19805-4811-4bf7-88f6-f35c06a6f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_tokens(tokens, n=1):\n",
    "    '''create a list of n-gram tokens from a list of tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens -- a list of tokens\n",
    "        n      -- the size of the window to use to build n-gram token list\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "        list of n-gram strings (whitespace separated) of length n\n",
    "    '''\n",
    "    \n",
    "    if n<2 or n>len(tokens):\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "        new_tokens.append(\" \".join(tokens[i:i+n]))\n",
    "        \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4adb735-0f59-47d9-8da9-97d1ff72a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kwic(kwic, win=None):\n",
    "    '''A basic print function for a KWIC object\n",
    "    \n",
    "    Args:\n",
    "        kwic -- a list of KWIC lines of the form [ [left words], kw, [right words]]\n",
    "        win  -- if None then use all words provided in context otherwise limit by win\n",
    "        \n",
    "    Prints KWIC lines with left context width/padding win*8 characters\n",
    "    '''\n",
    "    \n",
    "    if not kwic:\n",
    "        return\n",
    "    \n",
    "    if win is None:\n",
    "        win = len(kwic[0][0])\n",
    "\n",
    "    max_left = max([len(' '.join(line[0])) for line in kwic])+1\n",
    "\n",
    "        \n",
    "    for lnum,line in enumerate(kwic,1):\n",
    "        print(\"{:>2}.{:>{}.{}}  {}  {}\".format(lnum, ' '.join(line[0][-win:]).encode().decode('unicode-escape'), \n",
    "                                      max_left, max_left,\n",
    "                                      line[1], \n",
    "                                      ' '.join(line[2][:win]).encode().decode('unicode-escape')\n",
    "                                     )\n",
    "             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a15fe-2dc8-4d34-a0e6-173f5b127892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_items(dist1, dist2, items, scaling=10000, dp=2):\n",
    "    ''' given two Counter objects with common keys compare the frequency and relative frequency of list of items\n",
    "    \n",
    "    Args:\n",
    "        dist1    -- Counter frequency list object\n",
    "        dist2    -- Counter frequency list object\n",
    "        items    -- list of string items that should be keys in dist1 and dist2\n",
    "        scaling  -- normalization factor, e.g. 10,000 words (default: 100000)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of tuples of form\n",
    "            (item, item_freq_dist1, norm_item_freq_dist1, item_freq_dist2, norm_item_freq_dist2)\n",
    "    '''\n",
    "    dist1_size = sum(dist1.values())\n",
    "    dist2_size = sum(dist2.values())\n",
    "\n",
    "    item_comparison = []\n",
    "    \n",
    "    for item in items:\n",
    "        \n",
    "        d1_freq = dist1.get(item,0)\n",
    "        d2_freq = dist2.get(item,0)\n",
    "        \n",
    "        item_comparison.append((item, \n",
    "                                d1_freq, round(d1_freq/dist1_size*scaling, dp),\n",
    "                                d2_freq, round(d2_freq/dist2_size*scaling, dp)))\n",
    "    \n",
    "    return item_comparison\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d3127-95a5-4ca3-bf48-5d3f3fdeedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(comparison_data, label1='corpus 1', label2='corpus 2'):\n",
    "    ''' create a paired barplot of relative frequencies of items in two corpora\n",
    "    \n",
    "    Args:\n",
    "        comparison_data --  list of tuples produced by the compare_items() function\n",
    "        label1          --  legend label for first corpus (default: corpus 1)\n",
    "        label2          --  legend label for second corpus (default: corpus 2)\n",
    "        \n",
    "    Produces a Seaborn barplot\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    \n",
    "    df=pd.DataFrame(comparison_data)[[0,2,4]] \n",
    "    df.columns = ['item', label1, label2]\n",
    "    df2=df.melt(id_vars=['item'])\n",
    "    df2.columns=['item', 'corpus', 'frequency']\n",
    "    sn.barplot(x='item',y='frequency', hue='corpus',data=df2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e991a9-b3fd-4d05-ae90-990901c2af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_localized_rhyme(story_title, story_text, spaces):\n",
    "    chars_to_strip = ',.\\xa0:-()\\';$\"/?][!`Ą@Ś§¨’–“”…ï‘>&\\\\%˝˘*'\n",
    "\n",
    "    title = story_title\n",
    "    tokens = tokenize(story_text.lower(), stripchars=chars_to_strip)\n",
    "    \n",
    "    localized = []\n",
    "    \n",
    "    for index, token in enumerate(tokens):\n",
    "        \n",
    "        start = index - spaces\n",
    "        while start < 0:\n",
    "            start += 1\n",
    "            \n",
    "        stop = index + spaces + 1\n",
    "        while stop > len(tokens):\n",
    "            stop -= 1\n",
    "        \n",
    "        before = tokens[start:index]\n",
    "        after = tokens[index +  1:stop]\n",
    "        chunk = before + after\n",
    "\n",
    "        word_freq = len([word for word in chunk if word == token]) # list comprehension: for word in chunk, append word if word == token; brackets represent a list, so take the length of the list as that represents the count, aka frequency, of the word\n",
    "        \n",
    "        rhyme = [word for word in chunk if word in pronouncing.rhymes(token)]\n",
    "        rhyme_freq = len(rhyme) # list comprehension: for word in chunk, append word if word is in list of words that rhyme with token; brackets represent a list, so take the length of the list as that represents the count, aka frequency, of the word\n",
    "        rhyme_unique_freq = len(set(rhyme))\n",
    "        \n",
    "        row = [title, token, chunk, word_freq, rhyme_freq, rhyme_unique_freq]\n",
    "        localized.append(row)\n",
    "\n",
    "    return localized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e066c3-77e1-42f5-a335-0c482c412f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_localized_dataframe(localized):\n",
    "    title_column = [row[0] for row in localized]\n",
    "    token_column = [row[1] for row in localized] # more list comprehension: append variable(row), for variable(row) in list(localized)\n",
    "    chunk_column = [\", \".join(row[2]) for row in localized] # join function turns a list into a string using identified delimiter\n",
    "    word_freq_column = [row[3] for row in localized]\n",
    "    rhyme_freq_column = [row[4] for row in localized]\n",
    "    rhyme_unique_freq_column = [row[5] for row in localized]\n",
    "    \n",
    "    data = {\n",
    "        \"Title\": title_column,\n",
    "        \"Token\": token_column,\n",
    "        \"Chunk\": chunk_column,\n",
    "        \"Word Frequency\": word_freq_column,\n",
    "        \"Rhyme Frequency\": rhyme_freq_column,\n",
    "        \"Unique Rhyme Frequency\": rhyme_unique_freq_column\n",
    "    }\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
